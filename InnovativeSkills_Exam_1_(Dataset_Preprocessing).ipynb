{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install emoji\n",
        "%pip install spellchecker\n",
        "%pip install nltk\n",
        "%pip install contractions\n",
        "%pip install gingerit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yd9FwAnkW8Bt",
        "outputId": "8087dd21-4541-4324-9803-b9aed45a993f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: spellchecker in /usr/local/lib/python3.11/dist-packages (0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spellchecker) (75.2.0)\n",
            "Requirement already satisfied: inexactsearch in /usr/local/lib/python3.11/dist-packages (from spellchecker) (1.0.2)\n",
            "Requirement already satisfied: soundex>=1.0 in /usr/local/lib/python3.11/dist-packages (from inexactsearch->spellchecker) (1.1.3)\n",
            "Requirement already satisfied: silpa-common>=0.3 in /usr/local/lib/python3.11/dist-packages (from inexactsearch->spellchecker) (0.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Collecting gingerit\n",
            "  Downloading gingerit-0.0.0.1.tar.gz (966 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gingerit\n",
            "  Building wheel for gingerit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gingerit: filename=gingerit-0.0.0.1-py3-none-any.whl size=1305 sha256=f41829e631b14a955898b5342f9358229d9a37881d2501e0fedf30e509fb5797\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/4d/e8/4e9e60cc5892b405032e3d0f044da1f757240e945b4fd5c100\n",
            "Successfully built gingerit\n",
            "Installing collected packages: gingerit\n",
            "Successfully installed gingerit-0.0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import contractions\n",
        "import re\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "corpus = [\n",
        "    'Hey, can u plz tell me whereâ€™s my order??',\n",
        "\n",
        "    'i didnâ€™t receive my parcel yet!!!!',\n",
        "\n",
        "    'Whrâ€™s my ordr ðŸ˜¡ðŸ˜¡',\n",
        "\n",
        "    'delivery late af... i want refund now'\n",
        "]\n",
        "\n",
        "# Lowercasing: using list comprehension to lowercase the data by using the built in python function lower()\n",
        "corpus = [text.lower() for text in corpus]\n",
        "print(corpus)\n",
        "\n",
        "# Expanding Contractions: this is to transform contracted words like can't, don't etc. Such words will be epanded to cannot, do not respectively.\n",
        "corpus = [contractions.fix(text) for text in corpus]\n",
        "print(\"Removed Contractions: \",corpus)\n",
        "\n",
        "# Expanding abbreviations and removing emojis: created a map from the given data to expand slangs\n",
        "abbreviation_map = {\n",
        "    'u': 'you',\n",
        "    'plz': 'please',\n",
        "    'whr': 'where',\n",
        "    'af': 'as fuck' # found the meaning from google.\n",
        "}\n",
        "def expand_abbreviations(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)  # extract clean words which also removes emojis.\n",
        "    expanded_words = [abbreviation_map.get(word, word) for word in words]\n",
        "    return \" \".join(expanded_words)\n",
        "\n",
        "corpus = [expand_abbreviations(text) for text in corpus]\n",
        "print(\"Expanded Abbreviations: \",corpus)\n",
        "\n",
        "# Removing Punctuation\n",
        "corpus = [text.translate(str.maketrans('', '', string.punctuation)) for text in corpus]\n",
        "print(\"Removed Punctuations: \",corpus)\n",
        "\n",
        "# Correcting Spells: using the textblob library missspelled words are fixed.\n",
        "from textblob import TextBlob\n",
        "corpus = [str(TextBlob(text).correct()) for text in corpus]\n",
        "print(\"Spells Corrected: \",corpus)\n",
        "\n",
        "# Tokenization\n",
        "tokenized_corpus = [word_tokenize(text) for text in corpus]\n",
        "print(tokenized_corpus)\n",
        "\n",
        "# Removing StopWords: downloaded english stopwords from the nltk.corpus library are used to remove stopwords.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_corpus = [[word for word in doc if word not in stop_words] for doc in tokenized_corpus]\n",
        "print(filtered_corpus)\n",
        "\n",
        "\n",
        "# Lemmatization: The WordNetLemmatizer module from nltk.stem is used to transform words into their base form. haven't used stemming as lemmatization is more accurate then stemming.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in filtered_corpus]\n",
        "print(lemmatized_corpus)\n",
        "\n",
        "filtered_corpus = [' '.join(doc) for doc in filtered_corpus]\n",
        "print(\"Last: \",filtered_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4QgRSvuplF__",
        "outputId": "4eb2e5ca-948e-4767-f31c-fa0b68fbae01"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hey, can u plz tell me whereâ€™s my order??', 'i didnâ€™t receive my parcel yet!!!!', 'whrâ€™s my ordr ðŸ˜¡ðŸ˜¡', 'delivery late af... i want refund now']\n",
            "Removed Contractions:  ['hey, can you plz tell me where is my order??', 'i did not receive my parcel yet!!!!', 'whrâ€™s my ordr ðŸ˜¡ðŸ˜¡', 'delivery late af... i want refund now']\n",
            "Expanded Abbreviations:  ['hey can you please tell me where is my order', 'i did not receive my parcel yet', 'where s my ordr', 'delivery late as fuck i want refund now']\n",
            "Removed Punctuations:  ['hey can you please tell me where is my order', 'i did not receive my parcel yet', 'where s my ordr', 'delivery late as fuck i want refund now']\n",
            "Spells Corrected:  ['hey can you please tell me where is my order', 'i did not receive my parcel yet', 'where s my order', 'delivery late as fuck i want refund now']\n",
            "[['hey', 'can', 'you', 'please', 'tell', 'me', 'where', 'is', 'my', 'order'], ['i', 'did', 'not', 'receive', 'my', 'parcel', 'yet'], ['where', 's', 'my', 'order'], ['delivery', 'late', 'as', 'fuck', 'i', 'want', 'refund', 'now']]\n",
            "[['hey', 'please', 'tell', 'order'], ['receive', 'parcel', 'yet'], ['order'], ['delivery', 'late', 'fuck', 'want', 'refund']]\n",
            "[['hey', 'pleas', 'tell', 'order'], ['receiv', 'parcel', 'yet'], ['order'], ['deliveri', 'late', 'fuck', 'want', 'refund']]\n",
            "[['hey', 'please', 'tell', 'order'], ['receive', 'parcel', 'yet'], ['order'], ['delivery', 'late', 'fuck', 'want', 'refund']]\n",
            "Last:  ['hey please tell order', 'receive parcel yet', 'order', 'delivery late fuck want refund']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To preserve emotional intensity in the dataset we can keep the emojis and translate them into text so that the model can categorize different emotion.\n",
        "# We can do this before expanding the abbreviations so that they are translated properly.\n",
        "import emoji\n",
        "corpus = [\n",
        "    'Hey, can u plz tell me whereâ€™s my order?',\n",
        "\n",
        "    'i didnâ€™t receive my parcel yet!!!!',\n",
        "\n",
        "    'Whrâ€™s my ordr ðŸ˜¡ðŸ˜¡',\n",
        "\n",
        "    'delivery late af... i want refund now'\n",
        "]\n",
        "emoji_corpus = [emoji.demojize(doc, delimiters=(\" <\", \"> \")) for doc in corpus]\n",
        "print(emoji_corpus)\n",
        "\n",
        "# We can also add tags to repetative punctuations like !!! -> <exclaim> and so on and keep the delimeters <> intact in the dataset\n",
        "def tag_emotional_punctuation(text):\n",
        "    text = re.sub(r'!{2,}', ' <exclaim> ', text)       # 2+ exclamation marks\n",
        "    text = re.sub(r'\\?{2,}', ' <question> ', text)     # 2+ question marks\n",
        "    text = re.sub(r'\\.{2,}', ' <pause> ', text)        # 2+ periods\n",
        "    return text\n",
        "corpus = [tag_emotional_punctuation(text) for text in emoji_corpus]\n",
        "\n",
        "# emoji and punctuation tag delimeters (<>) are kept while removing punctuations.\n",
        "preserve_punct = \"<>\"\n",
        "remove_punct = ''.join([p for p in string.punctuation if p not in preserve_punct])\n",
        "corpus = [text.translate(str.maketrans('', '', remove_punct)) for text in corpus]\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnPg6sU5Wo_Z",
        "outputId": "89b93319-91f8-4214-8d39-b9a641f20420"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey, can u plz tell me whereâ€™s my order?', 'i didnâ€™t receive my parcel yet!!!!', 'Whrâ€™s my ordr  <enraged_face>  <enraged_face> ', 'delivery late af... i want refund now']\n",
            "['Hey can u plz tell me whereâ€™s my order', 'i didnâ€™t receive my parcel yet <exclaim> ', 'Whrâ€™s my ordr  <enragedface>  <enragedface> ', 'delivery late af <pause>  i want refund now']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhAxe_sEZ-IY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}